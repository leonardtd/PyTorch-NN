{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74eab0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5392/2098000290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1318c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a9dd7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_it = iter(train_loader)\n",
    "samples, labels = train_it.next()\n",
    "samples.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02085fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ee812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Module(nn.Module):\n",
    "    def __init__(self, in_channels, emb_dims):\n",
    "        super().__init__()\n",
    "        self.out_channels = emb_dims\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1) #1x28x28 -> MP 2x2\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) #32x14x14\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(32, self.out_channels, kernel_size=3, padding=1) #64x7x7\n",
    "        self.av1 = nn.AvgPool2d((7,7))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.mp1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.mp2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.av1(x)\n",
    "        x = x.reshape(-1,self.out_channels)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1a660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN_Module(in_channels=1, emb_dims=64)\n",
    "cnn(samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Layer(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(in_dims, out_dims)\n",
    "        \n",
    "    def forward(self, adjacency_mat, feature_mat):\n",
    "        x = adjacency_mat @ feature_mat #N, in_dims\n",
    "        x = F.leaky_relu(self.lin(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b30ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn = GCN_Layer(64,128)\n",
    "\n",
    "adj = torch.randn(16, 16)\n",
    "features = torch.randn(16,64)\n",
    "\n",
    "gcn(adj,features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCN(nn.Module):\n",
    "    def __init__(self, in_channels, emb_dims, in_dims, out_dims, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.emb_dims = emb_dims\n",
    "        self.in_dims = in_dims\n",
    "        self.out_dims = out_dims\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.adj_out_dims = 256\n",
    "        \n",
    "        self.cnn = CNN_Module(in_channels,emb_dims)\n",
    "        self.gcn1 = GCN_Layer(in_dims, in_dims)\n",
    "        self.gcn2 = GCN_Layer(in_dims, out_dims)\n",
    "        \n",
    "        self.adjc_fc = nn.Linear(emb_dims, self.adj_out_dims) #En el paper\n",
    "        self.features_fc = nn.Linear(emb_dims, in_dims)\n",
    "        \n",
    "        self.fc1 = nn.Linear(emb_dims+out_dims, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        \n",
    "        #Loss & Optim\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings, out = self.get_gcn_embeddings(x)\n",
    "        out = torch.cat([embeddings, out], dim=-1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_gcn_embeddings(self, x):\n",
    "        embeddings = self.cnn(x) #bs, h\n",
    "        \n",
    "        # Adjacency matrix\n",
    "        adj_linear = self.adjc_fc(embeddings)\n",
    "        adjacency_matrix = F.softmax(adj_linear, dim=-1)@(F.softmax(adj_linear,dim=-1).transpose(0,1))\n",
    "        adjacency_matrix = torch.div(adjacency_matrix,torch.linalg.norm(adjacency_matrix, dim=-1))\n",
    "        \n",
    "        # Feature matrix\n",
    "        feature_matrix = self.features_fc(embeddings)\n",
    "        \n",
    "        #GCN \n",
    "        out = F.relu(self.gcn1(adjacency_matrix, feature_matrix))\n",
    "        out = self.gcn2(adjacency_matrix, out)\n",
    "        return embeddings, out\n",
    "    \n",
    "    def get_final_embeddings(self, x):\n",
    "        embeddings, out = self.get_gcn_embeddings(x)\n",
    "        out = cat_tensor = torch.cat([embeddings, out], dim=-1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        return out\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        \n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(logits, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.cpu().item()\n",
    "    \n",
    "    def get_accuracy(self, x, targets):\n",
    "        logits = self.forward(x)\n",
    "        probs = torch.softmax(logits,dim=1)\n",
    "        predictions = torch.argmax(probs, dim=1)\n",
    "        return (torch.sum(predictions==targets).item())/float(targets.size()[0])\n",
    "    \n",
    "    def get_adj_mat(self, x):\n",
    "        embeddings = self.cnn(x) #bs, h\n",
    "        \n",
    "        # Adjacency matrix\n",
    "        adj_linear = self.adjc_fc(embeddings)\n",
    "        adjacency_matrix = F.softmax(adj_linear, dim=-1)@(F.softmax(adj_linear,dim=-1).transpose(0,1))\n",
    "        adjacency_matrix = torch.div(adjacency_matrix,torch.linalg.norm(adjacency_matrix, dim=-1))\n",
    "        \n",
    "        return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b206a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcgn = TGCN(1,128,256,512,10)\n",
    "tcgn(samples).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d9d81",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861044b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples, test_labels = next(iter(test_loader))\n",
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19404685",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcgn = TGCN(in_channels=1,emb_dims=2048, in_dims=256, out_dims=512,num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "num_epochs = 20\n",
    "n_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8d240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, step 100/469, train loss = 1.9973\n",
      "epoch 1 / 20, step 200/469, train loss = 1.5764\n",
      "epoch 1 / 20, step 300/469, train loss = 1.0119\n",
      "epoch 1 / 20, step 400/469, train loss = 0.8235\n",
      "epoch 2 / 20, step 100/469, train loss = 0.5446\n",
      "epoch 2 / 20, step 200/469, train loss = 0.4337\n",
      "epoch 2 / 20, step 300/469, train loss = 0.4353\n",
      "epoch 2 / 20, step 400/469, train loss = 0.4151\n",
      "epoch 3 / 20, step 100/469, train loss = 0.3358\n",
      "epoch 3 / 20, step 200/469, train loss = 0.1985\n",
      "epoch 3 / 20, step 300/469, train loss = 0.1891\n",
      "epoch 3 / 20, step 400/469, train loss = 0.1750\n",
      "epoch 4 / 20, step 100/469, train loss = 0.1422\n",
      "epoch 4 / 20, step 200/469, train loss = 0.2424\n",
      "epoch 4 / 20, step 300/469, train loss = 0.1860\n",
      "epoch 4 / 20, step 400/469, train loss = 0.3322\n",
      "epoch 5 / 20, step 100/469, train loss = 0.2099\n",
      "epoch 5 / 20, step 200/469, train loss = 0.2367\n",
      "epoch 5 / 20, step 300/469, train loss = 0.2113\n",
      "epoch 5 / 20, step 400/469, train loss = 0.1497\n",
      "epoch 6 / 20, step 100/469, train loss = 0.1611\n",
      "epoch 6 / 20, step 200/469, train loss = 0.1456\n",
      "epoch 6 / 20, step 300/469, train loss = 0.1982\n",
      "epoch 6 / 20, step 400/469, train loss = 0.1437\n",
      "epoch 7 / 20, step 100/469, train loss = 0.1735\n",
      "epoch 7 / 20, step 200/469, train loss = 0.0885\n",
      "epoch 7 / 20, step 300/469, train loss = 0.0454\n",
      "epoch 7 / 20, step 400/469, train loss = 0.0914\n",
      "epoch 8 / 20, step 100/469, train loss = 0.0552\n",
      "epoch 8 / 20, step 200/469, train loss = 0.0347\n",
      "epoch 8 / 20, step 300/469, train loss = 0.1685\n",
      "epoch 8 / 20, step 400/469, train loss = 0.1071\n",
      "epoch 9 / 20, step 100/469, train loss = 0.1705\n",
      "epoch 9 / 20, step 200/469, train loss = 0.1262\n",
      "epoch 9 / 20, step 300/469, train loss = 0.0605\n",
      "epoch 9 / 20, step 400/469, train loss = 0.1323\n",
      "epoch 10 / 20, step 100/469, train loss = 0.0664\n",
      "epoch 10 / 20, step 200/469, train loss = 0.0871\n",
      "epoch 10 / 20, step 300/469, train loss = 0.1001\n",
      "epoch 10 / 20, step 400/469, train loss = 0.0628\n",
      "epoch 11 / 20, step 100/469, train loss = 0.0542\n",
      "epoch 11 / 20, step 200/469, train loss = 0.0829\n",
      "epoch 11 / 20, step 300/469, train loss = 0.1211\n",
      "epoch 11 / 20, step 400/469, train loss = 0.0747\n",
      "epoch 12 / 20, step 100/469, train loss = 0.0660\n",
      "epoch 12 / 20, step 200/469, train loss = 0.0255\n",
      "epoch 12 / 20, step 300/469, train loss = 0.0650\n",
      "epoch 12 / 20, step 400/469, train loss = 0.1064\n",
      "epoch 13 / 20, step 100/469, train loss = 0.0361\n",
      "epoch 13 / 20, step 200/469, train loss = 0.0726\n",
      "epoch 13 / 20, step 300/469, train loss = 0.0635\n",
      "epoch 13 / 20, step 400/469, train loss = 0.0750\n",
      "epoch 14 / 20, step 100/469, train loss = 0.0455\n",
      "epoch 14 / 20, step 200/469, train loss = 0.0848\n",
      "epoch 14 / 20, step 300/469, train loss = 0.0444\n",
      "epoch 14 / 20, step 400/469, train loss = 0.0846\n",
      "epoch 15 / 20, step 100/469, train loss = 0.0554\n",
      "epoch 15 / 20, step 200/469, train loss = 0.0211\n",
      "epoch 15 / 20, step 300/469, train loss = 0.0448\n",
      "epoch 15 / 20, step 400/469, train loss = 0.1011\n",
      "epoch 16 / 20, step 100/469, train loss = 0.0102\n",
      "epoch 16 / 20, step 200/469, train loss = 0.0505\n",
      "epoch 16 / 20, step 300/469, train loss = 0.0141\n",
      "epoch 16 / 20, step 400/469, train loss = 0.0090\n",
      "epoch 17 / 20, step 100/469, train loss = 0.0287\n",
      "epoch 17 / 20, step 200/469, train loss = 0.0549\n",
      "epoch 17 / 20, step 300/469, train loss = 0.0170\n",
      "epoch 17 / 20, step 400/469, train loss = 0.0976\n",
      "epoch 18 / 20, step 100/469, train loss = 0.0338\n",
      "epoch 18 / 20, step 200/469, train loss = 0.0337\n",
      "epoch 18 / 20, step 300/469, train loss = 0.0162\n",
      "epoch 18 / 20, step 400/469, train loss = 0.0916\n",
      "epoch 19 / 20, step 100/469, train loss = 0.0312\n",
      "epoch 19 / 20, step 200/469, train loss = 0.0343\n",
      "epoch 19 / 20, step 300/469, train loss = 0.0318\n",
      "epoch 19 / 20, step 400/469, train loss = 0.1321\n",
      "epoch 20 / 20, step 100/469, train loss = 0.0319\n",
      "epoch 20 / 20, step 200/469, train loss = 0.0496\n",
      "epoch 20 / 20, step 300/469, train loss = 0.0280\n",
      "epoch 20 / 20, step 400/469, train loss = 0.0227\n"
     ]
    }
   ],
   "source": [
    "cum_loss = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "writer.add_graph(tcgn, test_samples.to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1,1,28,28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward\n",
    "        train_loss = tcgn.train_step(images, labels)\n",
    "        cum_loss += train_loss\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_steps}, train loss = {train_loss:.4f}')\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        test_samples = test_samples.to(device)\n",
    "        gcn_embeddings, _ = tcgn.get_gcn_embeddings(test_samples)\n",
    "        final_embeddings = tcgn.get_final_embeddings(test_samples)\n",
    "        \n",
    "        train_acc = tcgn.get_accuracy(test_samples, test_labels.to(device))\n",
    "        \n",
    "        writer.add_scalar('Loss/train',cum_loss/n_steps,epoch)\n",
    "        writer.add_scalar('Accuracy/test',train_acc,epoch)\n",
    "        \n",
    "        print(gcn_embeddings.shape, final_embeddings.shape, len(test_labels), test_samples.shape)\n",
    "        writer.add_embedding(gcn_embeddings, metadata=test_labels,label_img=test_samples,global_step=epoch, tag=f\"GCN Embeddings\")\n",
    "        writer.add_embedding(final_embeddings, metadata=test_labels,label_img=test_samples,global_step=epoch, tag=f\"Final Embeddings\")\n",
    "    \n",
    "    cum_loss = 0\n",
    "    \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
